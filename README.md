# ğŸ“Š Distributed Data Processing Pipeline

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg)](https://www.docker.com/)
[![Prometheus](https://img.shields.io/badge/Prometheus-2.48-E6522C.svg)](https://prometheus.io/)
[![Apache Spark](https://img.shields.io/badge/Apache Spark-3.5-E25A1C.svg)](https://spark.apache.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

[English](#english) | [PortuguÃªs](#portuguÃªs)

---

## English

### ğŸ¯ Overview

**Distributed Data Processing Pipeline** â€” Enterprise-grade distributed data processing pipeline with Apache Spark (Scala + Python), Delta Lake, and Airflow orchestration

Total source lines: **4,707** across **28** files in **3** languages.

### âœ¨ Key Features

- **Production-Ready Architecture**: Modular, well-documented, and following best practices
- **Comprehensive Implementation**: Complete solution with all core functionality
- **Clean Code**: Type-safe, well-tested, and maintainable codebase
- **Easy Deployment**: Docker support for quick setup and deployment

### ğŸš€ Quick Start

#### Prerequisites
- Python 3.12+
- Docker and Docker Compose (optional)

#### Installation

1. **Clone the repository**
```bash
git clone https://github.com/galafis/distributed-data-processing-pipeline.git
cd distributed-data-processing-pipeline
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```




## ğŸ³ Docker

```bash
# Build and start
docker-compose up -d

# View logs
docker-compose logs -f

# Stop
docker-compose down
```

### ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov --cov-report=html

# Run with verbose output
pytest -v
```

### ğŸ“ Project Structure

```
distributed-data-processing-pipeline/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pipeline.yaml
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ data_pipeline_dag.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ staging/
â”œâ”€â”€ docker/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ project/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_quality_report.py
â”‚   â”œâ”€â”€ run_integration_tests.sh
â”‚   â”œâ”€â”€ run_performance_tests.sh
â”‚   â””â”€â”€ validate_project.sh
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â””â”€â”€ scala/
â”‚   â””â”€â”€ test/
â”‚       â””â”€â”€ scala/
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ python/
â”‚       â”œâ”€â”€ integration/
â”‚       â”œâ”€â”€ unit/
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ AUDIT_FINAL_REPORT.md
â”œâ”€â”€ AUDIT_SUMMARY.md
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ DOCUMENTATION.md
â”œâ”€â”€ README.md
â””â”€â”€ docker-compose.yml
```

### ğŸ› ï¸ Tech Stack

| Technology | Usage |
|------------|-------|
| Python | 13 files |
| Scala | 12 files |
| Shell | 3 files |

### ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### ğŸ‘¤ Author

**Gabriel Demetrios Lafis**

- GitHub: [@galafis](https://github.com/galafis)
- LinkedIn: [Gabriel Demetrios Lafis](https://linkedin.com/in/gabriel-demetrios-lafis)

---

## PortuguÃªs

### ğŸ¯ VisÃ£o Geral

**Distributed Data Processing Pipeline** â€” Enterprise-grade distributed data processing pipeline with Apache Spark (Scala + Python), Delta Lake, and Airflow orchestration

Total de linhas de cÃ³digo: **4,707** em **28** arquivos em **3** linguagens.

### âœ¨ Funcionalidades Principais

- **Arquitetura Pronta para ProduÃ§Ã£o**: Modular, bem documentada e seguindo boas prÃ¡ticas
- **ImplementaÃ§Ã£o Completa**: SoluÃ§Ã£o completa com todas as funcionalidades principais
- **CÃ³digo Limpo**: Type-safe, bem testado e manutenÃ­vel
- **FÃ¡cil ImplantaÃ§Ã£o**: Suporte Docker para configuraÃ§Ã£o e implantaÃ§Ã£o rÃ¡pidas

### ğŸš€ InÃ­cio RÃ¡pido

#### PrÃ©-requisitos
- Python 3.12+
- Docker e Docker Compose (opcional)

#### InstalaÃ§Ã£o

1. **Clone the repository**
```bash
git clone https://github.com/galafis/distributed-data-processing-pipeline.git
cd distributed-data-processing-pipeline
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```




### ğŸ§ª Testes

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov --cov-report=html

# Run with verbose output
pytest -v
```

### ğŸ“ Estrutura do Projeto

```
distributed-data-processing-pipeline/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pipeline.yaml
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ data_pipeline_dag.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ staging/
â”œâ”€â”€ docker/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ project/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_quality_report.py
â”‚   â”œâ”€â”€ run_integration_tests.sh
â”‚   â”œâ”€â”€ run_performance_tests.sh
â”‚   â””â”€â”€ validate_project.sh
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â””â”€â”€ scala/
â”‚   â””â”€â”€ test/
â”‚       â””â”€â”€ scala/
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ python/
â”‚       â”œâ”€â”€ integration/
â”‚       â”œâ”€â”€ unit/
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ AUDIT_FINAL_REPORT.md
â”œâ”€â”€ AUDIT_SUMMARY.md
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ DOCUMENTATION.md
â”œâ”€â”€ README.md
â””â”€â”€ docker-compose.yml
```

### ğŸ› ï¸ Stack TecnolÃ³gica

| Tecnologia | Uso |
|------------|-----|
| Python | 13 files |
| Scala | 12 files |
| Shell | 3 files |

### ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

### ğŸ‘¤ Autor

**Gabriel Demetrios Lafis**

- GitHub: [@galafis](https://github.com/galafis)
- LinkedIn: [Gabriel Demetrios Lafis](https://linkedin.com/in/gabriel-demetrios-lafis)
