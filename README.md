# ğŸ“Š Distributed Data Processing Pipeline

> Enterprise-grade distributed data processing pipeline with Apache Spark (Scala + Python), Delta Lake, and Airflow orchestration

[![Python](https://img.shields.io/badge/Python-3.12-3776AB.svg)](https://img.shields.io/badge/)
[![Gin](https://img.shields.io/badge/Gin-1.9-00ADD8.svg)](https://img.shields.io/badge/)
[![NumPy](https://img.shields.io/badge/NumPy-1.26-013243.svg)](https://img.shields.io/badge/)
[![Pandas](https://img.shields.io/badge/Pandas-2.2-150458.svg)](https://img.shields.io/badge/)
[![Prometheus](https://img.shields.io/badge/Prometheus-2.48-E6522C.svg)](https://img.shields.io/badge/)
[![Apache_Spark](https://img.shields.io/badge/Apache_Spark-3.5-E25A1C.svg)](https://img.shields.io/badge/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

[English](#english) | [PortuguÃªs](#portuguÃªs)

---

## English

### ğŸ¯ Overview

**Distributed Data Processing Pipeline** is a production-grade Python application complemented by Scala, Shell that showcases modern software engineering practices including clean architecture, comprehensive testing, containerized deployment, and CI/CD readiness.

The codebase comprises **4,707 lines** of source code organized across **28 modules**, following industry best practices for maintainability, scalability, and code quality.

### âœ¨ Key Features

- **ğŸ”„ Data Pipeline**: Scalable ETL with parallel processing
- **âœ… Data Validation**: Schema validation and quality checks
- **ğŸ“Š Monitoring**: Pipeline health metrics and alerting
- **ğŸ”§ Configurability**: YAML/JSON-based pipeline configuration
- **ğŸ—ï¸ Object-Oriented**: 11 core classes with clean architecture

### ğŸ—ï¸ Architecture

```mermaid
graph TB
    subgraph Client["ğŸ–¥ï¸ Client Layer"]
        A[Web Client]
        B[API Documentation]
    end
    
    subgraph API["âš¡ API Layer"]
        C[Middleware Pipeline]
        D[Route Handlers]
        E[Business Logic]
    end
    
    subgraph Data["ğŸ’¾ Data Layer"]
        F[(Primary Database)]
        G[Cache]
    end
    
    A --> C
    B --> C
    C --> D --> E
    E --> F
    E --> G
    
    style Client fill:#e1f5fe
    style API fill:#f3e5f5
    style Data fill:#fff3e0
```

```mermaid
classDiagram
    class BatchETLJobSpec
    class DatabaseUtils
    class BaseSparkJobSpec
    class DataQualityChecker
    class MetricsCollector
    class S3Utils
    class QualityCheckResult
    class SparkJobRunner
    DatabaseUtils <|-- BatchETLJobSpec
    DatabaseUtils <|-- DataQualityChecker
    DatabaseUtils <|-- MetricsCollector
    DatabaseUtils <|-- S3Utils
    DatabaseUtils <|-- QualityCheckResult
    DatabaseUtils <|-- SparkJobRunner
    BaseSparkJobSpec <|-- BatchETLJobSpec
    BaseSparkJobSpec <|-- DataQualityChecker
    BaseSparkJobSpec <|-- MetricsCollector
    BaseSparkJobSpec <|-- S3Utils
    BaseSparkJobSpec <|-- QualityCheckResult
    BaseSparkJobSpec <|-- SparkJobRunner
```

### ğŸš€ Quick Start

#### Prerequisites

- Python 3.12+
- pip (Python package manager)
- Docker and Docker Compose (optional)

#### Installation

```bash
# Clone the repository
git clone https://github.com/galafis/distributed-data-processing-pipeline.git
cd distributed-data-processing-pipeline

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### Running

```bash
# Run the application
python src/main.py
```

### ğŸ³ Docker

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop all services
docker-compose down

# Rebuild after changes
docker-compose up -d --build
```

### ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage report
pytest --cov --cov-report=html

# Run specific test module
pytest tests/test_main.py -v

# Run with detailed output
pytest -v --tb=short
```

### ğŸ“ Project Structure

```
distributed-data-processing-pipeline/
â”œâ”€â”€ config/        # Configuration
â”‚   â””â”€â”€ pipeline.yaml
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ data_pipeline_dag.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ staging/
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ notebooks/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_quality_report.py
â”‚   â”œâ”€â”€ run_integration_tests.sh
â”‚   â”œâ”€â”€ run_performance_tests.sh
â”‚   â””â”€â”€ validate_project.sh
â”œâ”€â”€ src/          # Source code
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â””â”€â”€ scala/
â”‚   â””â”€â”€ test/         # Test suite
â”‚       â””â”€â”€ scala/
â”œâ”€â”€ tests/         # Test suite
â”‚   â””â”€â”€ python/
â”‚       â”œâ”€â”€ integration/
â”‚       â”œâ”€â”€ unit/
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ AUDIT_FINAL_REPORT.md
â”œâ”€â”€ AUDIT_SUMMARY.md
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ DOCUMENTATION.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

### ğŸ› ï¸ Tech Stack

| Technology | Description | Role |
|------------|-------------|------|
| **Python** | Core Language | Primary |
| **Gin** | Go web framework | Framework |
| **NumPy** | Numerical computing | Framework |
| **Pandas** | Data manipulation library | Framework |
| **Prometheus** | Monitoring & alerting | Framework |
| **Apache Spark** | Distributed computing | Framework |
| Scala | 12 files | Supporting |
| Shell | 3 files | Supporting |

### ğŸš€ Deployment

#### Cloud Deployment Options

The application is containerized and ready for deployment on:

| Platform | Service | Notes |
|----------|---------|-------|
| **AWS** | ECS, EKS, EC2 | Full container support |
| **Google Cloud** | Cloud Run, GKE | Serverless option available |
| **Azure** | Container Instances, AKS | Enterprise integration |
| **DigitalOcean** | App Platform, Droplets | Cost-effective option |

```bash
# Production build
docker build -t distributed-data-processing-pipeline:latest .

# Tag for registry
docker tag distributed-data-processing-pipeline:latest registry.example.com/distributed-data-processing-pipeline:latest

# Push to registry
docker push registry.example.com/distributed-data-processing-pipeline:latest
```

### ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

1. Fork the project
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### ğŸ‘¤ Author

**Gabriel Demetrios Lafis**
- GitHub: [@galafis](https://github.com/galafis)
- LinkedIn: [Gabriel Demetrios Lafis](https://linkedin.com/in/gabriel-demetrios-lafis)

---

## PortuguÃªs

### ğŸ¯ VisÃ£o Geral

**Distributed Data Processing Pipeline** Ã© uma aplicaÃ§Ã£o Python de nÃ­vel profissional, complementada por Scala, Shell que demonstra prÃ¡ticas modernas de engenharia de software, incluindo arquitetura limpa, testes abrangentes, implantaÃ§Ã£o containerizada e prontidÃ£o para CI/CD.

A base de cÃ³digo compreende **4,707 linhas** de cÃ³digo-fonte organizadas em **28 mÃ³dulos**, seguindo as melhores prÃ¡ticas do setor para manutenibilidade, escalabilidade e qualidade de cÃ³digo.

### âœ¨ Funcionalidades Principais

- **ğŸ”„ Data Pipeline**: Scalable ETL with parallel processing
- **âœ… Data Validation**: Schema validation and quality checks
- **ğŸ“Š Monitoring**: Pipeline health metrics and alerting
- **ğŸ”§ Configurability**: YAML/JSON-based pipeline configuration
- **ğŸ—ï¸ Object-Oriented**: 11 core classes with clean architecture

### ğŸ—ï¸ Arquitetura

```mermaid
graph TB
    subgraph Client["ğŸ–¥ï¸ Client Layer"]
        A[Web Client]
        B[API Documentation]
    end
    
    subgraph API["âš¡ API Layer"]
        C[Middleware Pipeline]
        D[Route Handlers]
        E[Business Logic]
    end
    
    subgraph Data["ğŸ’¾ Data Layer"]
        F[(Primary Database)]
        G[Cache]
    end
    
    A --> C
    B --> C
    C --> D --> E
    E --> F
    E --> G
    
    style Client fill:#e1f5fe
    style API fill:#f3e5f5
    style Data fill:#fff3e0
```

### ğŸš€ InÃ­cio RÃ¡pido

#### Prerequisites

- Python 3.12+
- pip (Python package manager)
- Docker and Docker Compose (optional)

#### Installation

```bash
# Clone the repository
git clone https://github.com/galafis/distributed-data-processing-pipeline.git
cd distributed-data-processing-pipeline

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### Running

```bash
# Run the application
python src/main.py
```

### ğŸ³ Docker

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop all services
docker-compose down

# Rebuild after changes
docker-compose up -d --build
```

### ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage report
pytest --cov --cov-report=html

# Run specific test module
pytest tests/test_main.py -v

# Run with detailed output
pytest -v --tb=short
```

### ğŸ“ Estrutura do Projeto

```
distributed-data-processing-pipeline/
â”œâ”€â”€ config/        # Configuration
â”‚   â””â”€â”€ pipeline.yaml
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ data_pipeline_dag.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ staging/
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ notebooks/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_quality_report.py
â”‚   â”œâ”€â”€ run_integration_tests.sh
â”‚   â”œâ”€â”€ run_performance_tests.sh
â”‚   â””â”€â”€ validate_project.sh
â”œâ”€â”€ src/          # Source code
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â””â”€â”€ scala/
â”‚   â””â”€â”€ test/         # Test suite
â”‚       â””â”€â”€ scala/
â”œâ”€â”€ tests/         # Test suite
â”‚   â””â”€â”€ python/
â”‚       â”œâ”€â”€ integration/
â”‚       â”œâ”€â”€ unit/
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ AUDIT_FINAL_REPORT.md
â”œâ”€â”€ AUDIT_SUMMARY.md
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ DOCUMENTATION.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

### ğŸ› ï¸ Stack TecnolÃ³gica

| Tecnologia | DescriÃ§Ã£o | Papel |
|------------|-----------|-------|
| **Python** | Core Language | Primary |
| **Gin** | Go web framework | Framework |
| **NumPy** | Numerical computing | Framework |
| **Pandas** | Data manipulation library | Framework |
| **Prometheus** | Monitoring & alerting | Framework |
| **Apache Spark** | Distributed computing | Framework |
| Scala | 12 files | Supporting |
| Shell | 3 files | Supporting |

### ğŸš€ Deployment

#### Cloud Deployment Options

The application is containerized and ready for deployment on:

| Platform | Service | Notes |
|----------|---------|-------|
| **AWS** | ECS, EKS, EC2 | Full container support |
| **Google Cloud** | Cloud Run, GKE | Serverless option available |
| **Azure** | Container Instances, AKS | Enterprise integration |
| **DigitalOcean** | App Platform, Droplets | Cost-effective option |

```bash
# Production build
docker build -t distributed-data-processing-pipeline:latest .

# Tag for registry
docker tag distributed-data-processing-pipeline:latest registry.example.com/distributed-data-processing-pipeline:latest

# Push to registry
docker push registry.example.com/distributed-data-processing-pipeline:latest
```

### ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para enviar um Pull Request.

### ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

### ğŸ‘¤ Autor

**Gabriel Demetrios Lafis**
- GitHub: [@galafis](https://github.com/galafis)
- LinkedIn: [Gabriel Demetrios Lafis](https://linkedin.com/in/gabriel-demetrios-lafis)
