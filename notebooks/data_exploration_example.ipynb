{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Data Processing Pipeline - Example Notebook\n",
    "\n",
    "This notebook demonstrates how to use the data processing pipeline for exploratory data analysis.\n",
    "\n",
    "**Author:** Gabriel Demetrios Lafis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import necessary libraries and initialize Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataExploration\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Data\n",
    "\n",
    "Create sample transaction data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate sample data\n",
    "data = []\n",
    "for i in range(1000):\n",
    "    data.append((\n",
    "        f\"tx_{i}\",\n",
    "        f\"cust_{random.randint(1, 100)}\",\n",
    "        f\"prod_{random.randint(1, 50)}\",\n",
    "        round(random.uniform(10.0, 500.0), 2),\n",
    "        random.randint(1, 10),\n",
    "        int((datetime.now() - timedelta(days=random.randint(0, 30))).timestamp()),\n",
    "        random.choice([\"US\", \"UK\", \"CA\", \"AU\", \"DE\"]),\n",
    "        random.choice([\"Electronics\", \"Books\", \"Clothing\", \"Food\", \"Home\"])\n",
    "    ))\n",
    "\n",
    "columns = [\"transactionId\", \"customerId\", \"productId\", \"amount\", \"quantity\", \"timestamp\", \"country\", \"category\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(f\"Generated {df.count()} transactions\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Transformation\n",
    "\n",
    "Apply transformations to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add derived columns\n",
    "df_transformed = df \\\n",
    "    .withColumn(\"total_amount\", col(\"amount\") * col(\"quantity\")) \\\n",
    "    .withColumn(\"date\", to_date(from_unixtime(col(\"timestamp\")))) \\\n",
    "    .withColumn(\"year\", year(col(\"date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"date\")))\n",
    "\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Checks\n",
    "\n",
    "Perform data quality validations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls\n",
    "print(\"Null counts:\")\n",
    "df_transformed.select([count(when(col(c).isNull(), c)).alias(c) for c in df_transformed.columns]).show()\n",
    "\n",
    "# Check for duplicates\n",
    "total_records = df_transformed.count()\n",
    "distinct_records = df_transformed.select(\"transactionId\").distinct().count()\n",
    "print(f\"\\nTotal records: {total_records}\")\n",
    "print(f\"Distinct records: {distinct_records}\")\n",
    "print(f\"Duplicates: {total_records - distinct_records}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregations\n",
    "\n",
    "Create summary statistics and aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by country and category\n",
    "df_agg = df_transformed.groupBy(\"country\", \"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        avg(\"total_amount\").alias(\"avg_revenue\"),\n",
    "        countDistinct(\"customerId\").alias(\"unique_customers\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Create visualizations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for visualization\n",
    "df_pandas = df_agg.toPandas()\n",
    "\n",
    "# Revenue by country\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "revenue_by_country = df_pandas.groupby('country')['total_revenue'].sum().sort_values(ascending=False)\n",
    "revenue_by_country.plot(kind='bar', color='steelblue')\n",
    "plt.title('Total Revenue by Country')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Revenue')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Revenue by category\n",
    "plt.subplot(1, 2, 2)\n",
    "revenue_by_category = df_pandas.groupby('category')['total_revenue'].sum().sort_values(ascending=False)\n",
    "revenue_by_category.plot(kind='bar', color='coral')\n",
    "plt.title('Total Revenue by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Revenue')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write Results to Delta Lake\n",
    "\n",
    "Save processed data to Delta Lake format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Delta Lake\n",
    "output_path = \"/tmp/delta-output/transactions\"\n",
    "\n",
    "df_transformed.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"country\", \"category\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(f\"Data written to: {output_path}\")\n",
    "\n",
    "# Verify the write\n",
    "df_read = spark.read.format(\"delta\").load(output_path)\n",
    "print(f\"Records written: {df_read.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Delta Lake Features\n",
    "\n",
    "Demonstrate Delta Lake capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Read Delta table\n",
    "deltaTable = DeltaTable.forPath(spark, output_path)\n",
    "\n",
    "# Show history\n",
    "print(\"Delta table history:\")\n",
    "deltaTable.history().select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)\n",
    "\n",
    "# Show table details\n",
    "print(\"\\nTable details:\")\n",
    "deltaTable.detail().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup\n",
    "\n",
    "Stop Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Setting up a Spark session with Delta Lake\n",
    "- Generating and transforming data\n",
    "- Performing data quality checks\n",
    "- Creating aggregations\n",
    "- Visualizing results\n",
    "- Writing to Delta Lake\n",
    "- Using Delta Lake features\n",
    "\n",
    "For more examples, see the [README](../README.md) and [code examples](../src/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
